# Example workflow using the OllamaAI plugin
# This workflow demonstrates how to use the OllamaAI plugin to generate AI responses

name: OllamaAI Example Workflow
description: |
  This workflow demonstrates how to use the OllamaAI plugin to generate AI responses from Ollama.
  It includes examples of using different models and passing log filter output to the AI.

# Define workflow execution sequence
sequence:
  # Example 1: Basic Ollama usage with llama3
  - name: Generate Response with Llama3
    nodeStep: true
    type: ollamaai-plugin
    configuration:
      ollamaModel: llama3
      ollamaServerUrl: http://localhost:11434
      prompt: |
        Write a short explanation of what Rundeck is and how it can be used for automation.
      temperature: 0.7
      maxTokens: 500
      topP: 1.0

  # Example 2: Using a different model (mistral)
  - name: Generate Response with Mistral
    nodeStep: true
    type: ollamaai-plugin
    configuration:
      ollamaModel: mistral
      prompt: |
        Explain the concept of Infrastructure as Code (IaC) and list 3 popular tools.
      temperature: 0.7
      maxTokens: 500
      topP: 1.0

  # Example 3: Using a custom model
  - name: Generate Response with Custom Model
    nodeStep: true
    type: ollamaai-plugin
    configuration:
      ollamaModel: custom-model
      customModelName: my-fine-tuned-model
      prompt: |
        Explain the benefits of containerization for application deployment.
      temperature: 0.7
      maxTokens: 500
      topP: 1.0

  # Example 4: Using log filter output with Ollama
  - name: Run a command that generates logs
    exec: |
      echo "Starting application..."
      echo "INFO: Initializing components"
      echo "INFO: Loading configuration from /etc/config.json"
      echo "ERROR: Failed to connect to database at db.example.com:5432"
      echo "ERROR: Connection timeout after 30 seconds"
      echo "WARN: Falling back to local cache"
      echo "INFO: Application started in degraded mode"

  - name: Analyze logs with Ollama
    nodeStep: true
    type: ollamaai-plugin
    configuration:
      ollamaModel: llama3
      prompt: |
        Analyze the following log output and identify any issues.
        Provide a brief explanation of what might be happening and suggest possible solutions.
      logFilterOutput: ${data.exec.output}
      temperature: 0.7
      maxTokens: 800
      topP: 1.0

  # Example 5: Using the AI response in a subsequent step
  - name: Use AI response in another step
    exec: |
      echo "AI Analysis Result:"
      echo ${data.ollamaai.response}
      
      # Parse the JSON response to extract just the text
      # In a real workflow, you might use jq or another tool to parse the JSON
      echo "Processing the AI response for further automation..."